{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Face Detection Tutorial\n",
    "<font color=\"red\"><b>This is NOT an Official Google Product and is only for education!!!</b></font>\n",
    "<br>\n",
    "In this sample, you'll use the Google Cloud Vision API to detect faces in an image. You'll then use that data to draw a box around each face using a helper function provided in utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "## Import Google Cloud Libraries for vision \n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "\n",
    "## To get Helpers\n",
    "from utils import helper\n",
    "from utils import detect\n",
    "\n",
    "## To Draw Image\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Face Detection API\n",
    "To construct a request to the Vision API, first consult the [API documentation](https://cloud.google.com/vision/docs/apis) and [How-To guides](https://cloud.google.com/vision/docs/how-to). In this case, you'll be asking the images resource to annotate your image. A request to this API takes the form of an object with a requests list. Each item in this list contains two bits of information:\n",
    "* The base64-encoded image data - [How to Base64 Encode Tutorial](https://cloud.google.com/vision/docs/base64)\n",
    "* A list of features you'd like annotated about that image - [All Capabilities of Vision API](https://cloud.google.com/vision/)\n",
    "\n",
    "For this example, you'll simply request FACE_DETECTION annotation on one image, and return the relevant portion of the response. The response to our face annotation request includes a bunch of metadata about the detected faces, which include coordinates of a polygon encompassing the face. To learn more about the response please visit [this link](https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#AnnotateImageResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def detect_face(face_file):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    content = face_file.read()\n",
    "    image = types.Image(content=content)\n",
    "    return client.face_detection(image=image).face_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Make the Request & Process the Response\n",
    "\n",
    "Lets make the request for an image and lets dive into the response. The response to our face annotation request includes a bunch of metadata about the detected faces, which include coordinates of a polygon encompassing the face. At this point, though, this is only a list of numbers. Let's use them to confirm that you have, in fact, found the faces in your image. We'll draw polygons onto a copy of the image, using the coordinates returned by the Vision API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Making the request\n",
    "imager = open('./face_detection.jpg', 'rb')\n",
    "faces = detect_face(imager)\n",
    "print('Found {} face{}'.format(len(faces), '' if len(faces) == 1 else 's'))\n",
    "\n",
    "# Reset the file pointer, so we can read the file again\n",
    "imager.seek(0)\n",
    "\n",
    "# Make a call to helper in our utils directory which build a new image & annotates it (faces) by drawing polygons for coordinates \n",
    "new_image = helper.highlight_faces(imager, faces)\n",
    "new_image.save('./face_detection_output.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw our Response\n",
    "Lets draw our response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face_img = io.imread('./face_detection_output.jpeg')\n",
    "io.imshow(face_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Bonus Lab\n",
    "<br>\n",
    "Look at ./utils/detect.py file. It has all features of vision API read to invoke. Feel free to try things like landmark, logos, safe_search, etc. See landmark example below to get going. If you feel adventurous, you can even take on a [Kaggle Challenge](https://www.kaggle.com/c/landmark-recognition-challenge#). Below is an image from that very challenge. \n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://lh3.googleusercontent.com/-qsiWGuoACYw/TKDGd3lfDQI/AAAAAAAATg0/RghgKG0aekE/s1600/\" width=\"500\" align='left'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets make a call to detect landmarks\n",
    "detect.detect_landmarks_uri('https://lh3.googleusercontent.com/-qsiWGuoACYw/TKDGd3lfDQI/AAAAAAAATg0/RghgKG0aekE/s1600/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
